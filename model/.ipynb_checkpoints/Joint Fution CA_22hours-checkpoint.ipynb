{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeline: (21540, 132)\n",
      "baseline: (21540, 70)\n",
      "timeline: (42320, 22, 6)\n",
      "baseline: (42320, 70)\n",
      "label: (42320,)\n",
      "timeline: (5384, 22, 6)\n",
      "baseline: (5384, 70)\n",
      "label: (5384, 1)\n",
      "timeline_no_smote: (21540, 22, 6)\n",
      "baseline_no_smote: (21540, 70)\n",
      "label_no_smote: (21540, 1)\n",
      "timeline_nr: (760, 22, 6)\n",
      "label_nr: (760,)\n"
     ]
    }
   ],
   "source": [
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(2)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(24)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(26)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "#from keras import backend as K\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "\n",
    "from sklearn import preprocessing  #用來標準化刻度\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import logcosh, categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Import Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from time import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "####################################################################################  x_lstm_validation\n",
    "\n",
    "T=22\n",
    "\n",
    "train_cardiac_total=pd.read_csv(\"mimic_ca_vital_sign_train_\"+str(T)+\"hours.csv\")\n",
    "test_cardiac_total=pd.read_csv(\"mimic_ca_vital_sign_test_\"+str(T)+\"hours.csv\")\n",
    "train_cardiac_base_total=pd.read_csv(\"mimic_ca_baseline_total_v2.csv\")\n",
    "\n",
    "eicu_cardiac_total=pd.read_csv(\"eicu_total_\"+str(T)+\"hours.csv\")\n",
    "\n",
    "total_train=21540 #control+event\n",
    "total_test=5384 #control+event\n",
    "train_control=21160 #control\n",
    "\n",
    "\n",
    "var=6\n",
    "random=32\n",
    "smote_ratio=1\n",
    "near_ratio=1\n",
    "EPOCH = 3                    # number of epochs\n",
    "BATCH = 32                      # batch size\n",
    "\n",
    "dropout=0.4\n",
    "LR = 0.001                           # learning rate of the gradient descent\n",
    "LAMBD = 0.001                       # lambda in L2 regularizaion\n",
    "\n",
    "#####################################################################################\n",
    "train_cardiac_base_total=train_cardiac_base_total.drop(['subject_id'],axis=1)\n",
    "train_cardiac_base_total=train_cardiac_base_total.drop(['hadm_id'],axis=1)\n",
    "train_cardiac_base_total=train_cardiac_base_total.drop(['stay_id'],axis=1)\n",
    "train_cardiac_base_total=train_cardiac_base_total.drop(['los'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['CA'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['hospDIED'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['cardR'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['DNR'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['CMO'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['DNRDNI'],axis=1)\n",
    "\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['DNI'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['FullCode'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['indextime'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['ccs9'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['ccs10'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['cardRv2'],axis=1)\n",
    "\n",
    "###############CXR##############\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Atelectasis'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Cardiomegaly'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Consolidation'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Edema'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Enlarged Cardiomediastinum'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Fracture'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Lung Lesion'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Lung Opacity'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['No Finding'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Pleural Effusion'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Pleural Other'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Pneumonia'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Pneumothorax'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Support Devices'],axis=1)\n",
    "###############CXR##############\n",
    "\n",
    "#train_cardiac_base_total=pd.get_dummies(data=train_cardiac_base_total,columns=[\"first_careunit\",\"ethnicity\",\"BMI\"])\n",
    "train_cardiac_base_total=pd.get_dummies(data=train_cardiac_base_total,columns=[\"first_careunit\",\"ethnicity\"])\n",
    "\n",
    "\n",
    "####################################################################\n",
    "df_train_base=train_cardiac_base_total[:total_train]\n",
    "y_train=df_train_base[['eventV3']].values   #取train_labels\n",
    "y_train_nr=df_train_base[['eventV3']].values   #取train_labels\n",
    "y_train_base=df_train_base[['eventV3']].values   #取train_labels\n",
    "y_train_no_smote=df_train_base[['eventV3']].values   #取train_labels\n",
    "df_train_base=df_train_base.drop(['eventV3'],axis=1)\n",
    "train_features=df_train_base.values\n",
    "\n",
    "df_test_base=train_cardiac_base_total[total_train:]\n",
    "y_test=df_test_base[['eventV3']].values   #取test_labels\n",
    "y_test_log=df_test_base['eventV3'].values   #取test_labels\n",
    "df_test_base=df_test_base.drop(['eventV3'],axis=1)\n",
    "test_features=df_test_base.values\n",
    "\n",
    "minmax_scale =preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "x_train_base=minmax_scale.fit_transform(train_features)\n",
    "x_test_base=minmax_scale.fit_transform(test_features)\n",
    "\n",
    "x_train_base_no_smote=minmax_scale.fit_transform(train_features)\n",
    "\n",
    "#x_train_base=train_features\n",
    "#x_test_base=test_features\n",
    "\n",
    "sm = SMOTE(random_state=random, sampling_strategy=smote_ratio)\n",
    "nr = NearMiss(sampling_strategy=near_ratio) \n",
    "\n",
    "x_train_base, y_train_base = sm.fit_sample(x_train_base, y_train_base.ravel())\n",
    "#x_train_base, y_train_base = nr.fit_sample(x_train_base, y_train_base.ravel())\n",
    "\n",
    "train_cardiac_total=train_cardiac_total[['vHR','vRR','vsbp','vdbp','vmbp','vspo2']]    \n",
    "train_cardiac_total=np.array(train_cardiac_total).reshape(total_train,T*var) #轉二維  array\n",
    "train_cardiac_total= pd.DataFrame(train_cardiac_total)\n",
    "\n",
    "x_test_lstm=test_cardiac_total[['vHR','vRR','vsbp','vdbp','vmbp','vspo2']].values \n",
    "#x_test_lstm=minmax_scale.fit_transform(x_test_lstm)  #規一化\n",
    "x_test_lstm=np.array(x_test_lstm).reshape(total_test,T,var) \n",
    "\n",
    "x_train_lstm, y_train = sm.fit_sample(train_cardiac_total, y_train.ravel())\n",
    "\n",
    "x_train_lstm_nr, y_train_nr = nr.fit_sample(train_cardiac_total, y_train_nr.ravel())\n",
    "\n",
    "#x_train_lstm=minmax_scale.fit_transform(x_train_lstm)  #規一化\n",
    "\n",
    "x_train_lstm=np.array(x_train_lstm).reshape(x_train_lstm.shape[0],T,var) #轉三維  total\n",
    "\n",
    "x_train_lstm_nr=np.array(x_train_lstm_nr).reshape(x_train_lstm_nr.shape[0],T,var) #轉三維  total\n",
    "\n",
    "x_train_lstm_no_smote=np.array(train_cardiac_total).reshape(train_cardiac_total.shape[0],T,var) #轉三維  total\n",
    "\n",
    "print('timeline:',train_cardiac_total.shape)\n",
    "print('baseline:',df_train_base.shape)\n",
    "\n",
    "print('timeline:',x_train_lstm.shape)\n",
    "print('baseline:',x_train_base.shape)\n",
    "print('label:',y_train.shape)\n",
    "\n",
    "print('timeline:',x_test_lstm.shape)\n",
    "print('baseline:',x_test_base.shape)\n",
    "print('label:',y_test.shape)\n",
    "\n",
    "print('timeline_no_smote:',x_train_lstm_no_smote.shape)\n",
    "print('baseline_no_smote:',x_train_base_no_smote.shape)\n",
    "print('label_no_smote:',y_train_no_smote.shape)\n",
    "\n",
    "\n",
    "print('timeline_nr:',x_train_lstm_nr.shape)\n",
    "print('label_nr:',y_train_nr.shape)\n",
    "#print(df_train_base.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42320, 22, 6)\n",
      "(42320,)\n",
      "layers=[8, 8, 8, 1], train_examples=42320, test_examples=5384\n",
      "batch = 32, timesteps = 22, features = 6, epochs = 3\n",
      "lr = 0.001, lambda = 0.001, dropout = 1, recurr_dropout = 1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 22, 8)             480       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 8)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 22, 8)             32        \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 22, 8)             544       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 22, 8)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 22, 8)             32        \n",
      "_________________________________________________________________\n",
      "LAYERS (LSTM)                (None, 8)                 544       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,673\n",
      "Trainable params: 1,625\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 33856 samples, validate on 8464 samples\n",
      "Epoch 1/3\n",
      " - 48s - loss: 0.6235 - accuracy: 0.7073 - f1_m: 0.5298 - precision_m: 0.6866 - val_loss: 0.4928 - val_accuracy: 0.7303 - val_f1_m: 0.8420 - val_precision_m: 1.0000\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tony\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,val_f1_m,val_precision_m,loss,accuracy,f1_m,precision_m,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    }
   ],
   "source": [
    "print(x_train_lstm.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "LAYERS = [8,8,8,1]                # number of units in hidden and output layers\n",
    "M_TRAIN = x_train_lstm.shape[0]           # number of training examples (2D)\n",
    "#M_VALIDATION =x_valid_lstm_new.shape[0]  \n",
    "M_TEST = x_test_lstm.shape[0]             # number of test examples (2D),full=X_test.shape[0]\n",
    "N = x_train_lstm.shape[2]                 # number of features\n",
    "\n",
    "#BATCH = M_TRAIN                          # batch size\n",
    "DP = 1                            # dropout rate\n",
    "RDP = 1                          # recurrent dropout rate\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "# Build the Model\n",
    "model_smote = Sequential()\n",
    "\n",
    "model_smote.add(LSTM(input_shape=(T, N), units=LAYERS[0],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              # dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_smote.add(Dropout(dropout))\n",
    "model_smote.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_smote.add(LSTM(units=LAYERS[1],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              # dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_smote.add(Dropout(dropout))\n",
    "model_smote.add(BatchNormalization())\n",
    "\n",
    "model_smote.add(LSTM(units=LAYERS[2],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              # dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=False, return_state=False,\n",
    "               stateful=False, unroll=False,name=\"LAYERS\"\n",
    "              ))\n",
    "model_smote.add(Dropout(dropout))\n",
    "model_smote.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_smote.add(Dense(units=LAYERS[3], activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model_smote.compile(loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m], optimizer=Adam(lr=LR))\n",
    "\n",
    "print(model_smote.summary())\n",
    "\n",
    "# Define a learning rate decay method:\n",
    "lr_decay = ReduceLROnPlateau(monitor='loss', \n",
    "                             patience=1, verbose=0, \n",
    "                             factor=0.5, min_lr=1e-8)\n",
    "\n",
    "# Define Early Stopping:\n",
    "early_stop = EarlyStopping(monitor='val_acc', min_delta=0, \n",
    "                           patience=30, verbose=1, mode='auto',\n",
    "                           baseline=0, restore_best_weights=True)\n",
    "# Train the model. \n",
    "# The dataset is small for NN - let's use test_data for validation\n",
    "start = time()\n",
    "\n",
    "##################################################\n",
    "\n",
    "History = model_smote.fit(x_train_lstm, y_train,\n",
    "                    epochs=EPOCH,\n",
    "                    batch_size=BATCH,\n",
    "                    validation_split=0.2,\n",
    "                    #validation_data=(x_valid_lstm_new[:M_VALIDATION], y_valid_lstm_new[:M_VALIDATION]),\n",
    "                    #validation_data=(x_test_lstm[:M_TEST], y_test[:M_TEST]),\n",
    "                    shuffle=True,\n",
    "                    verbose=2,\n",
    "                    callbacks=[lr_decay, early_stop])\n",
    "\n",
    "print('-'*65)\n",
    "print(f'Training was completed in {time() - start:.2f} secs')\n",
    "print('-'*65)\n",
    "# Evaluate the model:\n",
    "train_loss, train_acc, train_f1_score, train_precision = model_smote.evaluate(x_train_lstm, y_train,\n",
    "                                       batch_size=BATCH, verbose=0)\n",
    "\n",
    "test_loss, test_acc, test_f1_score, test_precision = model_smote.evaluate(x_test_lstm[:M_TEST], y_test[:M_TEST],\n",
    "                                     batch_size=BATCH, verbose=0)\n",
    "print('-'*65)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "print(f'test error = {round((1 - test_acc) * M_TEST)} out of {M_TEST} examples')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_smote= model_smote.predict(x_test_lstm)\n",
    "\n",
    "predict_test_smote=[]\n",
    "for i in range(y_pred_smote.shape[0]): \n",
    "    if y_pred_smote[i]>0.5:\n",
    "        predict_test_smote.append(1)\n",
    "    else:\n",
    "        predict_test_smote.append(0)\n",
    "predict_test_smote = np.array(predict_test_smote)\n",
    "print(predict_test_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1D=np.array(y_test).reshape(total_test)\n",
    "\n",
    "#predict_train_lstm = model.predict(x_train_lstm)\n",
    "#predict_train_lstm=np.array(predict_train_lstm).reshape(total_train.shape[0]) #37536\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test_smote,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test_smote)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "accuracy= (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,1]+cm1[1,0])   #FPR\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "ppv =  cm1[1,1]/(cm1[0,1]+cm1[1,1])   \n",
    "npv =  cm1[0,0]/(cm1[0,0]+cm1[1,0])  \n",
    "\n",
    "print('accuracy:',accuracy)\n",
    "print('specificity:',specificity)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('ppv:',ppv)\n",
    "print('npv:',npv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr,tpr,threshold = roc_curve(y_test, y_pred_smote) ###計算真正率和假正率\n",
    "roc_auc = auc(fpr,tpr) ###計算auc的值\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "lw=lw, label='LSTM ROC curve (area = %0.2f)' % roc_auc) ###假正率為橫座標，真正率為縱座標做曲線\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('smote')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "get_11rd_layer_output = K.function([model_smote.layers[0].input],\n",
    "         [model_smote.layers[6].output])\n",
    "layer_output = get_11rd_layer_output([x_train_lstm])[0]\n",
    "\n",
    "layer_output_test = get_11rd_layer_output([x_test_lstm])[0]\n",
    "\n",
    "print(layer_output.shape)\n",
    "\n",
    "print(layer_output)\n",
    "\n",
    "print(layer_output_test.shape)\n",
    "\n",
    "print(layer_output_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_base_combine=np.hstack((x_train_base,layer_output))\n",
    "x_test_base_combine=np.hstack((x_test_base,layer_output_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_base_combine.shape)\n",
    "print(x_test_base_combine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc  ###計算roc和auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics,ensemble\n",
    "from sklearn import model_selection\n",
    "\n",
    "gbm = xgb.XGBClassifier()\n",
    "\n",
    "xgb_params = {\n",
    "#'learning_rate': [0.1,0.2,0.5],\n",
    "#'n_estimators': [30,50,100],\n",
    "#'max_depth': [5,10,20],\n",
    "'n_estimators': [3,4,5],\n",
    "'max_depth': [3,4,5]\n",
    "#'n_estimators': [3],\n",
    "#'max_depth': [3]\n",
    "#'n_estimators': [5],\n",
    "#'max_depth': [30]\n",
    "#'n_estimators': [30],\n",
    "#'max_depth': [5]\n",
    "#'n_estimators': [30],\n",
    "#'max_depth': [30]\n",
    "# 'alpha': [0.4,0.6],\n",
    " }\n",
    "\n",
    "xg_reg = model_selection.GridSearchCV(gbm, xgb_params, cv=5,scoring=\"roc_auc\")\n",
    "xg_reg.fit(x_train_base_combine,y_train)\n",
    "\n",
    "y_pred = xg_reg.predict(x_test_base_combine)\n",
    "\n",
    "y_pred = xg_reg.predict(x_test_base_combine)\n",
    "y_pred_xg = xg_reg.predict_proba(x_test_base_combine)\n",
    "y_pred_xg = y_pred_xg[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_and_score(y_test, pred_proba):\n",
    "    fpr, tpr, _ = roc_curve(y_test.ravel(), pred_proba.ravel())\n",
    "    roc_auc = roc_auc_score(y_test.ravel(), pred_proba.ravel())\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "\n",
    "predict_test=[]\n",
    "for i in range(y_pred_xg.shape[0]): \n",
    "    if y_pred_xg[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "y_test_1D=np.array(y_test).reshape(5384)\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "\n",
    "fpr,tpr,roc_auc = roc_curve_and_score(y_test, y_pred_xg) ###計算真正率和假正率\n",
    "accuracy=(cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
    "\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "print('AUC : %0.2f' % roc_auc)  #AUC\n",
    "\n",
    "print('accuracy :%0.2f' % accuracy)\n",
    "\n",
    "\n",
    "print(xg_reg.best_params_)\n",
    "print(xg_reg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics,ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "forest = ensemble.RandomForestClassifier()\n",
    "\n",
    "rf_params = {\n",
    "'n_estimators': [15,20,25],\n",
    "'max_depth': [4,5,6,7]\n",
    "#'n_estimators': [5],\n",
    "#'max_depth': [5]\n",
    " }\n",
    "\n",
    "forest = model_selection.GridSearchCV(forest, rf_params, cv=5)\n",
    "forest_fit=forest.fit(x_train_base_combine,y_train)\n",
    "\n",
    "\n",
    "prob_predict_y_validation = forest.predict_proba(x_test_base_combine)#给出带有概率值的结果，每个点所有label的概率和为1\n",
    "y_score = prob_predict_y_validation[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test=[]\n",
    "for i in range(y_score.shape[0]): \n",
    "    if y_score[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "y_test_1D=np.array(y_test).reshape(5384)\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "\n",
    "accuracy=(cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
    "\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "fpr,tpr,roc_auc = roc_curve_and_score(y_test, y_score) ###計算真正率和假正率\n",
    "\n",
    "print('AUC : %0.2f' % roc_auc)  #AUC\n",
    "\n",
    "print('accuracy :%0.2f' % accuracy)\n",
    "\n",
    "print(forest.best_params_)\n",
    "print(forest.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "logregCV = LogisticRegressionCV(cv=5)\n",
    "logregCV.fit(x_train_base_combine, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_test_y_predicted_pro_CV = logregCV.predict_proba(x_test_base_combine)\n",
    "\n",
    "log_score_CV = logreg_test_y_predicted_pro_CV[:, 1]\n",
    "\n",
    "predict_test_CV=[]\n",
    "for i in range(log_score_CV.shape[0]): \n",
    "    if log_score_CV[i]>0.5:\n",
    "        predict_test_CV.append(1)\n",
    "    else:\n",
    "        predict_test_CV.append(0)\n",
    "predict_test_CV = np.array(predict_test_CV)\n",
    "\n",
    "#print(predict_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score,confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test_CV)\n",
    "\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "\n",
    "#print('Precision:',precision_score(y_test_1, predict_test))\n",
    "#print('Recall:', recall_score(y_test_1, predict_test))\n",
    "#print('f1-score: %f' % f1_score(y_test_1, predict_test))\n",
    "print('Accuracy: %f' % accuracy_score(y_test, predict_test_CV))\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test_CV,rownames=['label'],colnames=['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(x_train_base_combine, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nei_test_y_predicted = neigh.predict(x_test_base_combine)\n",
    "predict_test=[]\n",
    "for i in range(nei_test_y_predicted.shape[0]): \n",
    "    if nei_test_y_predicted[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "#print(predict_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score,confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test)\n",
    "\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "\n",
    "#print('Precision:',precision_score(y_test_1, predict_test))\n",
    "#print('Recall:', recall_score(y_test_1, predict_test))\n",
    "#print('f1-score: %f' % f1_score(y_test_1, predict_test))\n",
    "print('Accuracy: %f' % accuracy_score(y_test, predict_test))\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test,rownames=['label'],colnames=['predict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import  svm, preprocessing, metrics \n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "svm = svm.SVC(probability=True)\n",
    "svm.fit(x_train_base_combine,y_train)\n",
    "svm_y_predicted = svm.predict(x_test_base_combine)\n",
    "\n",
    "svm_y_predicted_pro = svm.predict_proba(x_test_base_combine)\n",
    "\n",
    "svm_score=svm_y_predicted_pro[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test=[]\n",
    "for i in range(svm_score.shape[0]): \n",
    "    if svm_score[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "#print(predict_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score,confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test,predict_test)\n",
    "\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "\n",
    "#print('Precision:',precision_score(y_test_1, predict_test))\n",
    "#print('Recall:', recall_score(y_test_1, predict_test))\n",
    "#print('f1-score: %f' % f1_score(y_test_1, predict_test))\n",
    "print('Accuracy: %f' % accuracy_score(y_test, predict_test))\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test,rownames=['label'],colnames=['predict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc  ###計算roc和auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "#plt.grid()\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, log_score_CV)\n",
    "plt.plot(fpr, tpr, color='gray', lw=2,\n",
    "         label='Logistic Cardiac AUC={0:.2f}'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_pred_xg)\n",
    "plt.plot(fpr, tpr, color='#00db00', lw=2,\n",
    "         label='XGBoost Cardiac AUC={0:.2f}'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_score)\n",
    "plt.plot(fpr, tpr, color='#ff00ff', lw=2,\n",
    "         label='Random Forest Cardiac AUC={0:.2f}'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, nei_test_y_predicted)\n",
    "plt.plot(fpr, tpr, color='red', lw=2,\n",
    "         label='Nearest Neighbors Cardiac AUC={0:.2f}'.format(roc_auc))\n",
    "\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, svm_score)\n",
    "plt.plot(fpr, tpr, color='blue', lw=2,\n",
    "         label='SVM Cardiac AUC={0:.2f}'.format(roc_auc))\n",
    "\n",
    "plt.title('Cardiac arrest Multi-Model Add features')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
