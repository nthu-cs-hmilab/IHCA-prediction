{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42320, 84)\n",
      "(5384, 84)\n",
      "[0 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "(42320,)\n",
      "(5384,)\n"
     ]
    }
   ],
   "source": [
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(2)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(24)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(26)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "#from keras import backend as K\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing  #用來標準化刻度\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout   \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss \n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "df=pd.read_csv(\"mimic_ca_baseline_total_v2_with_cxr.csv\")   \n",
    "\n",
    "df=df.drop(['subject_id'],axis=1)\n",
    "df=df.drop(['hadm_id'],axis=1)\n",
    "df=df.drop(['stay_id'],axis=1)\n",
    "df=df.drop(['los'],axis=1)\n",
    "\n",
    "#df=df.drop(['CA'],axis=1)\n",
    "#df=df.drop(['hospDIED'],axis=1)\n",
    "#df=df.drop(['cardR'],axis=1)\n",
    "#df=df.drop(['DNR'],axis=1)\n",
    "#df=df.drop(['CMO'],axis=1)\n",
    "#df=df.drop(['DNRDNI'],axis=1)\n",
    "\n",
    "#df=df.drop(['DNI'],axis=1)\n",
    "#df=df.drop(['FullCode'],axis=1)\n",
    "#df=df.drop(['indextime'],axis=1)\n",
    "#df=df.drop(['ccs9'],axis=1)\n",
    "#df=df.drop(['ccs10'],axis=1)\n",
    "#df=df.drop(['cardRv2'],axis=1)\n",
    "\n",
    "###############CXR##############\n",
    "#df=df.drop(['Atelectasis'],axis=1)\n",
    "#df=df.drop(['Cardiomegaly'],axis=1)\n",
    "#df=df.drop(['Consolidation'],axis=1)\n",
    "#df=df.drop(['Edema'],axis=1)\n",
    "#df=df.drop(['Enlarged Cardiomediastinum'],axis=1)\n",
    "#df=df.drop(['Fracture'],axis=1)\n",
    "#df=df.drop(['Lung Lesion'],axis=1)\n",
    "#df=df.drop(['Lung Opacity'],axis=1)\n",
    "#df=df.drop(['No Finding'],axis=1)\n",
    "#df=df.drop(['Pleural Effusion'],axis=1)\n",
    "#df=df.drop(['Pleural Other'],axis=1)\n",
    "#df=df.drop(['Pneumonia'],axis=1)\n",
    "#df=df.drop(['Pneumothorax'],axis=1)\n",
    "#df=df.drop(['Support Devices'],axis=1)\n",
    "###############CXR##############\n",
    "\n",
    "def roc_curve_and_score(y_test, pred_proba):\n",
    "    fpr, tpr, _ = roc_curve(y_test.ravel(), pred_proba.ravel())\n",
    "    roc_auc = roc_auc_score(y_test.ravel(), pred_proba.ravel())\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "random=32\n",
    "smote_ratio=1\n",
    "\n",
    "#df=pd.get_dummies(data=df,columns=[\"first_careunit\",\"ethnicity\",\"BMI\"])\n",
    "df=pd.get_dummies(data=df,columns=[\"first_careunit\",\"ethnicity\"])\n",
    "\n",
    "df_train=df[:21540]\n",
    "df_test=df[21540:]\n",
    "\n",
    "#print(df.columns)\n",
    "\n",
    "y_train=df_train['eventV3']\n",
    "y_test=df_test['eventV3'].values\n",
    "\n",
    "df_train=df_train.drop(['eventV3'],axis=1)\n",
    "train_features=df_train.values\n",
    "df_test=df_test.drop(['eventV3'],axis=1)\n",
    "test_features=df_test.values\n",
    "\n",
    "minmax_scale =preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "X_train=minmax_scale.fit_transform(train_features)\n",
    "X_test=minmax_scale.fit_transform(test_features)\n",
    "\n",
    "#X_train=train_features\n",
    "#X_test=test_features\n",
    "\n",
    "sm = SMOTE(random_state=random,sampling_strategy=smote_ratio)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "#nr = NearMiss(sampling_strategy=0.35) \n",
    "#X_train, y_train = nr.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc  ###計算roc和auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics,ensemble\n",
    "from sklearn import model_selection\n",
    "\n",
    "gbm = xgb.XGBClassifier()\n",
    "\n",
    "xgb_params = {\n",
    "#'learning_rate': [0.1,0.2,0.5],\n",
    "#'n_estimators': [30,50,100],\n",
    "#'max_depth': [5,10,20],\n",
    "'n_estimators': [3,4,5],\n",
    "'max_depth': [3,4,5]\n",
    "#'n_estimators': [3],\n",
    "#'max_depth': [3]\n",
    "#'n_estimators': [5],\n",
    "#'max_depth': [30]\n",
    "#'n_estimators': [30],\n",
    "#'max_depth': [5]\n",
    "#'n_estimators': [30],\n",
    "#'max_depth': [30]\n",
    "# 'alpha': [0.4,0.6],\n",
    " }\n",
    "\n",
    "xg_reg = model_selection.GridSearchCV(gbm, xgb_params, cv=5,scoring=\"roc_auc\")\n",
    "xg_reg.fit(X_train,y_train)\n",
    "\n",
    "y_pred = xg_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[4927  362]\n",
      " [  51   44]]\n",
      "Sensitivity : 0.46\n",
      "Specificity :0.93\n",
      "AUC : 0.70\n",
      "accuracy :0.92\n",
      "{'max_depth': 5, 'n_estimators': 5}\n",
      "0.9807389676191123\n"
     ]
    }
   ],
   "source": [
    "predict_test=[]\n",
    "for i in range(y_pred.shape[0]): \n",
    "    if y_pred[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "y_test_1D=np.array(y_test).reshape(5384)\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "\n",
    "fpr,tpr,roc_auc = roc_curve_and_score(y_test, y_pred) ###計算真正率和假正率\n",
    "accuracy=(cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
    "\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "print('AUC : %0.2f' % roc_auc)  #AUC\n",
    "\n",
    "print('accuracy :%0.2f' % accuracy)\n",
    "\n",
    "\n",
    "print(xg_reg.best_params_)\n",
    "print(xg_reg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics,ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "forest = ensemble.RandomForestClassifier()\n",
    "\n",
    "rf_params = {\n",
    "'n_estimators': [15,20,25],\n",
    "'max_depth': [4,5,6,7]\n",
    "#'n_estimators': [5],\n",
    "#'max_depth': [5]\n",
    " }\n",
    "\n",
    "forest = model_selection.GridSearchCV(forest, rf_params, cv=5)\n",
    "forest_fit=forest.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "prob_predict_y_validation = forest.predict_proba(X_test)#给出带有概率值的结果，每个点所有label的概率和为1\n",
    "y_score = prob_predict_y_validation[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test=[]\n",
    "for i in range(y_score.shape[0]): \n",
    "    if y_score[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "y_test_1D=np.array(y_test).reshape(5384)\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "\n",
    "accuracy=(cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
    "\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "fpr,tpr,roc_auc = roc_curve_and_score(y_test, y_score) ###計算真正率和假正率\n",
    "\n",
    "print('AUC : %0.2f' % roc_auc)  #AUC\n",
    "\n",
    "print('accuracy :%0.2f' % accuracy)\n",
    "\n",
    "print(forest.best_params_)\n",
    "print(forest.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Optimal_Cutoff(TPR, FPR, threshold):\n",
    "    y = TPR - FPR\n",
    "    Youden_index = np.argmax(y)  # Only the first occurrence is returned.\n",
    "    optimal_threshold = threshold[Youden_index]\n",
    "    point = [FPR[Youden_index], TPR[Youden_index]]\n",
    "    return optimal_threshold, point\n",
    "def ROC(label, y_prob):\n",
    "    \"\"\"\n",
    "    Receiver_Operating_Characteristic, ROC\n",
    "    :param label: (n, )\n",
    "    :param y_prob: (n, )\n",
    "    :return: fpr, tpr, roc_auc, optimal_th, optimal_point\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(label, y_prob)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    optimal_th, optimal_point = Find_Optimal_Cutoff(TPR=tpr, FPR=fpr, threshold=thresholds)\n",
    "    return fpr, tpr, roc_auc, optimal_th, optimal_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Optimal_Cutoff_1(target, predicted):\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr)) \n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "    return list(roc_t['threshold']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal probability threshold\n",
    "threshold = Find_Optimal_Cutoff_1(y_test, y_score)\n",
    "print (threshold)\n",
    "\n",
    "predict_test_new=[]\n",
    "for i in range(y_score.shape[0]): \n",
    "    if y_score[i]>threshold:\n",
    "        predict_test_new.append(1)\n",
    "    else:\n",
    "        predict_test_new.append(0)\n",
    "predict_test_new = np.array(predict_test_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, roc_auc, optimal_th, optimal_point = ROC(y_test, y_score)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.plot(optimal_point[0], optimal_point[1], marker='o', color='r')\n",
    "plt.text(optimal_point[0], optimal_point[1], f'Threshold:{optimal_th:.2f}')\n",
    "plt.title(\"ROC-AUC\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "log_test_y_predicted = logreg.fit(X_train, y_train).decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_test_y_predicted = logreg.predict(X_test)\n",
    "predict_test=[]\n",
    "for i in range(logreg_test_y_predicted.shape[0]): \n",
    "    if logreg_test_y_predicted[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "#print(predict_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score,confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test,predict_test)\n",
    "\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "\n",
    "#print('Precision:',precision_score(y_test_1, predict_test))\n",
    "#print('Recall:', recall_score(y_test_1, predict_test))\n",
    "#print('f1-score: %f' % f1_score(y_test_1, predict_test))\n",
    "print('Accuracy: %f' % accuracy_score(y_test, predict_test))\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "pd.crosstab(y_test,predict_test,rownames=['label'],colnames=['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_test_y_predicted_pro = logreg.predict_proba(X_test)\n",
    "\n",
    "log_score = logreg_test_y_predicted_pro[:, 1]\n",
    "\n",
    "predict_test=[]\n",
    "for i in range(log_score.shape[0]): \n",
    "    if log_score[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "#print(predict_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score,confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test,predict_test)\n",
    "\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "\n",
    "#print('Precision:',precision_score(y_test_1, predict_test))\n",
    "#print('Recall:', recall_score(y_test_1, predict_test))\n",
    "#print('f1-score: %f' % f1_score(y_test_1, predict_test))\n",
    "print('Accuracy: %f' % accuracy_score(y_test, predict_test))\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "pd.crosstab(y_test,predict_test,rownames=['label'],colnames=['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nei_test_y_predicted = neigh.predict(X_test)\n",
    "predict_test=[]\n",
    "for i in range(nei_test_y_predicted.shape[0]): \n",
    "    if nei_test_y_predicted[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "#print(predict_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score,confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test,predict_test)\n",
    "\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "\n",
    "#print('Precision:',precision_score(y_test_1, predict_test))\n",
    "#print('Recall:', recall_score(y_test_1, predict_test))\n",
    "#print('f1-score: %f' % f1_score(y_test_1, predict_test))\n",
    "print('Accuracy: %f' % accuracy_score(y_test, predict_test))\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "pd.crosstab(y_test,predict_test,rownames=['label'],colnames=['predict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import  svm, preprocessing, metrics \n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "svm = svm.SVC(probability=True)\n",
    "svm.fit(X_train,y_train)\n",
    "svm_y_predicted = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_y_predicted_pro = svm.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_score=svm_y_predicted_pro[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test=[]\n",
    "for i in range(nei_test_y_predicted.shape[0]): \n",
    "    if nei_test_y_predicted[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "#print(predict_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score,confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test,predict_test)\n",
    "\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "\n",
    "#print('Precision:',precision_score(y_test_1, predict_test))\n",
    "#print('Recall:', recall_score(y_test_1, predict_test))\n",
    "#print('f1-score: %f' % f1_score(y_test_1, predict_test))\n",
    "print('Accuracy: %f' % accuracy_score(y_test, predict_test))\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "pd.crosstab(y_test,predict_test,rownames=['label'],colnames=['predict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc  ###計算roc和auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "#plt.grid()\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, log_score)\n",
    "plt.plot(fpr, tpr, color='gray', lw=2,\n",
    "         label='Logistic Cardiac AUC={0:.2f}'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, color='#00db00', lw=2,\n",
    "         label='XGBoost Cardiac AUC={0:.2f}'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_score)\n",
    "plt.plot(fpr, tpr, color='#ff00ff', lw=2,\n",
    "         label='Random Forest Cardiac AUC={0:.2f}'.format(roc_auc))\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, nei_test_y_predicted)\n",
    "plt.plot(fpr, tpr, color='red', lw=2,\n",
    "         label='Nearest Neighbors Cardiac AUC={0:.2f}'.format(roc_auc))\n",
    "\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, svm_score)\n",
    "plt.plot(fpr, tpr, color='blue', lw=2,\n",
    "         label='SVM Cardiac AUC={0:.2f}'.format(roc_auc))\n",
    "\n",
    "plt.title('Multi-Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('label:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original ROC area: {:0.3f}\".format(roc_auc_score(y_test,y_pred)))\n",
    "n_bootstraps = 1000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(y_pred), len(y_pred))\n",
    "    if len(np.unique(y_test[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(y_test[indices], y_pred[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "   # print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "    \n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "\n",
    "confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(bootstrapped_scores, bins=50)\n",
    "plt.title('Histogram of the bootstrapped ROC AUC scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC : %0.2f' % roc_auc)  #AUC\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "\n",
    "print(\"Original ROC area: {:0.3f}\".format(roc_auc_score(y_test,y_score)))\n",
    "n_bootstraps = 1000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(y_score), len(y_score))\n",
    "    if len(np.unique(y_test[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(y_test[indices], y_score[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "   # print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "    \n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "\n",
    "predict_test=[]\n",
    "for i in range(y_score.shape[0]): \n",
    "    if y_score[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "y_test_1D=np.array(y_test).reshape(5384)\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "\n",
    "print('AUC : %0.2f' % roc_auc)  #AUC\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "\n",
    "confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(bootstrapped_scores, bins=50)\n",
    "plt.title('Histogram of the bootstrapped ROC AUC scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(X, Y):\n",
    "    return 1/(len(X)*len(Y)) * sum([kernel(x, y) for x in X for y in Y])\n",
    "def kernel(X, Y):\n",
    "    return .5 if Y==X else int(Y < X)\n",
    "def structural_components(X, Y):\n",
    "    V10 = [1/len(Y) * sum([kernel(x, y) for y in Y]) for x in X]\n",
    "    V01 = [1/len(X) * sum([kernel(x, y) for x in X]) for y in Y]\n",
    "    return V10, V01\n",
    "    \n",
    "def get_S_entry(V_A, V_B, auc_A, auc_B):\n",
    "    return 1/(len(V_A)-1) * sum([(a-auc_A)*(b-auc_B) for a,b in zip(V_A, V_B)])\n",
    "def z_score(var_A, var_B, covar_AB, auc_A, auc_B):\n",
    "    return (auc_A - auc_B)/((var_A + var_B - 2*covar_AB)**(.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn import metrics\n",
    "# Model A (random) vs. \"good\" model B\n",
    "#preds_A = np.array([.5, .5, .5, .5, .5, .5, .5, .5, .5, .5])\n",
    "#preds_B = np.array([.2, .5, .1, .4, .9, .8, .7, .5, .9, .8])\n",
    "#actual= np.array([0, 0, 0, 0, 1, 0, 1, 1, 1, 1])\n",
    "preds_A = y_pred\n",
    "preds_B = y_score\n",
    "actual= y_test\n",
    "def group_preds_by_label(preds, actual):\n",
    "    X = [p for (p, a) in zip(preds, actual) if a]\n",
    "    Y = [p for (p, a) in zip(preds, actual) if not a]\n",
    "    return X, Y\n",
    "X_A, Y_A = group_preds_by_label(preds_A, actual)\n",
    "X_B, Y_B = group_preds_by_label(preds_B, actual)\n",
    "V_A10, V_A01 = structural_components(X_A, Y_A)\n",
    "V_B10, V_B01 = structural_components(X_B, Y_B)\n",
    "auc_A = auc(X_A, Y_A)\n",
    "auc_B = auc(X_B, Y_B)\n",
    "# Compute entries of covariance matrix S (covar_AB = covar_BA)\n",
    "var_A = (get_S_entry(V_A10, V_A10, auc_A, auc_A) * 1/len(V_A10)\n",
    "         + get_S_entry(V_A01, V_A01, auc_A, auc_A) * 1/len(V_A01))\n",
    "var_B = (get_S_entry(V_B10, V_B10, auc_B, auc_B) * 1/len(V_B10)\n",
    "         + get_S_entry(V_B01, V_B01, auc_B, auc_B) * 1/len(V_B01))\n",
    "covar_AB = (get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)\n",
    "            + get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "# Two tailed test\n",
    "z = z_score(var_A, var_B, covar_AB, auc_A, auc_B)\n",
    "p = st.norm.sf(abs(z))*2\n",
    "\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_idx = forest_fit.best_estimator_.feature_importances_.argsort()\n",
    "#print(importances)\n",
    "\n",
    "print(sorted_idx)\n",
    "\n",
    "#sorted_idx = forest.feature_importances_.argsort()\n",
    "\n",
    "top_10=sorted_idx[60:]\n",
    "\n",
    "plt.barh(df_train.columns[top_10], forest_fit.best_estimator_.feature_importances_[top_10])\n",
    "plt.xlabel(\"Random Forest Feature Importance\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
