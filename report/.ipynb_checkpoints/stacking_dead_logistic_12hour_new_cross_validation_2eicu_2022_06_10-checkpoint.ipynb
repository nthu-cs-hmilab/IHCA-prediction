{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(2)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(24)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(26)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "#from keras import backend as K\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "\n",
    "from sklearn import preprocessing  #用來標準化刻度\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import logcosh, categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Import Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from time import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "####################################################################################  x_lstm_validation\n",
    "\n",
    "T=12\n",
    "\n",
    "train_cardiac_total=pd.read_csv(\"mimic_dead_vital_sign_train_\"+str(T)+\"hours.csv\")\n",
    "test_cardiac_total=pd.read_csv(\"mimic_dead_vital_sign_test_\"+str(T)+\"hours.csv\")\n",
    "train_cardiac_base_total=pd.read_csv(\"mimic_dead_baseline_total_v2.csv\")\n",
    "\n",
    "eicu_cardiac_total=pd.read_csv(\"eicu_total_\"+str(T)+\"hours.csv\")\n",
    "\n",
    "total_train=24100 #control+event\n",
    "total_test=6025 #control+event\n",
    "train_control=21160 #control\n",
    "\n",
    "var=6\n",
    "random=32\n",
    "smote_ratio=1\n",
    "#near_ratio=0.35\n",
    "EPOCH = 3                    # number of epochs\n",
    "BATCH = 32                      # batch size\n",
    "\n",
    "dropout=0.4\n",
    "LR = 0.001                           # learning rate of the gradient descent\n",
    "LAMBD = 0.001                       # lambda in L2 regularizaion\n",
    "\n",
    "#####################################################################################\n",
    "train_cardiac_base_total=train_cardiac_base_total.drop(['subject_id'],axis=1)\n",
    "train_cardiac_base_total=train_cardiac_base_total.drop(['hadm_id'],axis=1)\n",
    "train_cardiac_base_total=train_cardiac_base_total.drop(['stay_id'],axis=1)\n",
    "train_cardiac_base_total=train_cardiac_base_total.drop(['los'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['CA'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['hospDIED'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['cardR'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['DNR'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['CMO'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['DNRDNI'],axis=1)\n",
    "\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['DNI'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['FullCode'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['indextime'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['ccs9'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['ccs10'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['cardRv2'],axis=1)\n",
    "\n",
    "###############CXR##############\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Atelectasis'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Cardiomegaly'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Consolidation'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Edema'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Enlarged Cardiomediastinum'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Fracture'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Lung Lesion'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Lung Opacity'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['No Finding'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Pleural Effusion'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Pleural Other'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Pneumonia'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Pneumothorax'],axis=1)\n",
    "#train_cardiac_base_total=train_cardiac_base_total.drop(['Support Devices'],axis=1)\n",
    "###############CXR##############\n",
    "\n",
    "#train_cardiac_base_total=pd.get_dummies(data=train_cardiac_base_total,columns=[\"first_careunit\",\"ethnicity\",\"BMI\"])\n",
    "train_cardiac_base_total=pd.get_dummies(data=train_cardiac_base_total,columns=[\"first_careunit\",\"ethnicity\"])\n",
    "\n",
    "\n",
    "####################################################################\n",
    "df_train_base=train_cardiac_base_total[:total_train]\n",
    "y_train=df_train_base[['eventV3']].values   #取train_labels\n",
    "y_train_base=df_train_base[['eventV3']].values   #取train_labels\n",
    "\n",
    "df_train_base=df_train_base.drop(['eventV3'],axis=1)\n",
    "train_features=df_train_base.values\n",
    "\n",
    "df_test_base=train_cardiac_base_total[total_train:]\n",
    "y_test=df_test_base[['eventV3']].values   #取test_labels\n",
    "\n",
    "y_test_log=df_test_base['eventV3'].values   #取test_labels\n",
    "\n",
    "df_test_base=df_test_base.drop(['eventV3'],axis=1)\n",
    "test_features=df_test_base.values\n",
    "\n",
    "minmax_scale =preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "x_train_base=minmax_scale.fit_transform(train_features)\n",
    "x_test_base=minmax_scale.fit_transform(test_features)\n",
    "\n",
    "#x_train_base=train_features\n",
    "#x_test_base=test_features\n",
    "\n",
    "sm = SMOTE(random_state=random, sampling_strategy=smote_ratio)\n",
    "#nr = NearMiss(sampling_strategy=near_ratio) \n",
    "\n",
    "x_train_base, y_train_base = sm.fit_sample(x_train_base, y_train_base.ravel())\n",
    "#x_train_base, y_train_base = nr.fit_sample(x_train_base, y_train_base.ravel())\n",
    "\n",
    "train_cardiac_total=train_cardiac_total[['vHR','vRR','vsbp','vdbp','vmbp','vspo2']]    \n",
    "train_cardiac_total=np.array(train_cardiac_total).reshape(total_train,T*var) #轉二維  array\n",
    "train_cardiac_total= pd.DataFrame(train_cardiac_total)\n",
    "\n",
    "x_test_lstm=test_cardiac_total[['vHR','vRR','vsbp','vdbp','vmbp','vspo2']].values \n",
    "#x_test_lstm=minmax_scale.fit_transform(x_test_lstm)  #規一化\n",
    "x_test_lstm=np.array(x_test_lstm).reshape(total_test,T,var) \n",
    "\n",
    "x_train_lstm, y_train = sm.fit_sample(train_cardiac_total, y_train.ravel())\n",
    "\n",
    "#x_train_lstm=minmax_scale.fit_transform(x_train_lstm)  #規一化\n",
    "\n",
    "x_train_lstm=np.array(x_train_lstm).reshape(x_train_lstm.shape[0],T,var) #轉三維  total \n",
    "\n",
    "print('timeline:',train_cardiac_total.shape)\n",
    "print('baseline:',df_train_base.shape)\n",
    "\n",
    "print('timeline:',x_train_lstm.shape)\n",
    "print('baseline:',x_train_base.shape)\n",
    "print('label:',y_train.shape)\n",
    "\n",
    "print('timeline:',x_test_lstm.shape)\n",
    "print('baseline:',x_test_base.shape)\n",
    "print('label:',y_test.shape)\n",
    "\n",
    "#print(df_train_base.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_event_0=x_train_lstm[:train_control]   #切出正常組 事件組   #loo1\n",
    "y_event_0=y_train[:train_control]\n",
    "\n",
    "x_event_1=x_train_lstm[train_control:]\n",
    "y_event_1=y_train[train_control:]\n",
    "\n",
    "part_0=int(int(x_event_0.shape[0])/5)\n",
    "part_1=int(int(x_event_1.shape[0])/5)\n",
    "\n",
    "x_train_lstm_new=np.concatenate((x_event_0[part_0:], x_event_1[part_1:]))\n",
    "x_valid_lstm_new=np.concatenate((x_event_0[:part_0], x_event_1[:part_1]))\n",
    "\n",
    "y_train_lstm_new=np.concatenate((y_event_0[part_0:], y_event_1[part_1:]))\n",
    "y_valid_lstm_new=np.concatenate((y_event_0[:part_0], y_event_1[:part_1]))\n",
    "\n",
    "print(x_train_lstm_new.shape)\n",
    "print(x_valid_lstm_new.shape)\n",
    "\n",
    "print(y_train_lstm_new.shape)\n",
    "print(y_valid_lstm_new.shape)\n",
    "\n",
    "LAYERS = [8,8,8,1]                # number of units in hidden and output layers\n",
    "M_TRAIN = x_train_lstm_new.shape[0]           # number of training examples (2D)\n",
    "M_VALIDATION =x_valid_lstm_new.shape[0]  \n",
    "M_TEST = x_test_lstm.shape[0]             # number of test examples (2D),full=X_test.shape[0]\n",
    "N = x_train_lstm.shape[2]                 # number of features\n",
    "\n",
    "#BATCH = M_TRAIN                          # batch size\n",
    "DP = 1                            # dropout rate\n",
    "RDP = 1                          # recurrent dropout rate\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "# Build the Model\n",
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(LSTM(input_shape=(T, N), units=LAYERS[0],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              # dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_1.add(Dropout(dropout))\n",
    "model_1.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_1.add(LSTM(units=LAYERS[1],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              # dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_1.add(Dropout(dropout))\n",
    "model_1.add(BatchNormalization())\n",
    "\n",
    "model_1.add(LSTM(units=LAYERS[2],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              # dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=False, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_1.add(Dropout(dropout))\n",
    "model_1.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_1.add(Dense(units=LAYERS[3], activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model_1.compile(loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m], optimizer=Adam(lr=LR))\n",
    "\n",
    "print(model_1.summary())\n",
    "\n",
    "# Define a learning rate decay method:\n",
    "lr_decay = ReduceLROnPlateau(monitor='loss', \n",
    "                             patience=1, verbose=0, \n",
    "                             factor=0.5, min_lr=1e-8)\n",
    "\n",
    "# Define Early Stopping:\n",
    "early_stop = EarlyStopping(monitor='val_acc', min_delta=0, \n",
    "                           patience=30, verbose=1, mode='auto',\n",
    "                           baseline=0, restore_best_weights=True)\n",
    "# Train the model. \n",
    "# The dataset is small for NN - let's use test_data for validation\n",
    "start = time()\n",
    "\n",
    "##################################################\n",
    "\n",
    "History = model_1.fit(x_train_lstm_new, y_train_lstm_new,\n",
    "                    epochs=EPOCH,\n",
    "                    batch_size=BATCH,\n",
    "                    validation_split=0,\n",
    "                    validation_data=(x_valid_lstm_new[:M_VALIDATION], y_valid_lstm_new[:M_VALIDATION]),\n",
    "                    #validation_data=(x_test_lstm[:M_TEST], y_test[:M_TEST]),\n",
    "                    shuffle=True,\n",
    "                    verbose=2,\n",
    "                    callbacks=[lr_decay, early_stop])\n",
    "\n",
    "print('-'*65)\n",
    "print(f'Training was completed in {time() - start:.2f} secs')\n",
    "print('-'*65)\n",
    "# Evaluate the model:\n",
    "train_loss, train_acc, train_f1_score, train_precision = model_1.evaluate(x_train_lstm_new, y_train_lstm_new,\n",
    "                                       batch_size=BATCH, verbose=0)\n",
    "\n",
    "test_loss, test_acc, test_f1_score, test_precision = model_1.evaluate(x_test_lstm[:M_TEST], y_test[:M_TEST],\n",
    "                                     batch_size=BATCH, verbose=0)\n",
    "print('-'*65)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "print(f'test error = {round((1 - test_acc) * M_TEST)} out of {M_TEST} examples')\n",
    "\n",
    "y_pred1= model_1.predict(x_test_lstm)\n",
    "predict_train_lstm1=model_1.predict(x_train_lstm)\n",
    "\n",
    "test_acc_1=test_acc\n",
    "test_precision_1=test_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_1=[]\n",
    "for i in range(y_pred1.shape[0]): \n",
    "    if y_pred1[i]>0.5:\n",
    "        predict_test_1.append(1)\n",
    "    else:\n",
    "        predict_test_1.append(0)\n",
    "predict_test_1 = np.array(predict_test_1)\n",
    "print(predict_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1D=np.array(y_test).reshape(total_test)\n",
    "\n",
    "#predict_train_lstm = model.predict(x_train_lstm)\n",
    "#predict_train_lstm=np.array(predict_train_lstm).reshape(total_train.shape[0]) #37536\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test_1,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test_1)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "ppv =  cm1[1,1]/(cm1[0,1]+cm1[1,1])   \n",
    "npv =  cm1[0,0]/(cm1[0,0]+cm1[1,0])  \n",
    "\n",
    "print('specificity:',specificity)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('ppv:',ppv)\n",
    "print('npv:',npv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_event_0=x_train_lstm[:train_control]   #切出正常組 事件組   #loo2\n",
    "y_event_0=y_train[:train_control]\n",
    "\n",
    "x_event_1=x_train_lstm[train_control:]\n",
    "y_event_1=y_train[train_control:]\n",
    "\n",
    "part_0=int(int(x_event_0.shape[0])/5)\n",
    "part_1=int(int(x_event_1.shape[0])/5)\n",
    "\n",
    "x_train_lstm_new=np.concatenate((x_event_0[:part_0], x_event_1[:part_1],x_event_0[2*part_0:],x_event_1[2*part_1:]))\n",
    "x_valid_lstm_new=np.concatenate((x_event_0[part_0:2*part_0], x_event_1[part_1:2*part_1]))\n",
    "\n",
    "y_train_lstm_new=np.concatenate((y_event_0[:part_0], y_event_1[:part_1],y_event_0[2*part_0:], y_event_1[2*part_1:]))\n",
    "y_valid_lstm_new=np.concatenate((y_event_0[part_0:2*part_0], y_event_1[part_1:2*part_1]))\n",
    "\n",
    "print(x_train_lstm_new.shape)\n",
    "print(x_valid_lstm_new.shape)\n",
    "\n",
    "print(y_train_lstm_new.shape)\n",
    "print(y_valid_lstm_new.shape)\n",
    "\n",
    "\n",
    "LAYERS = [8,8,8,1]                # number of units in hidden and output layers\n",
    "M_TRAIN = x_train_lstm_new.shape[0]           # number of training examples (2D)\n",
    "M_VALIDATION =x_valid_lstm_new.shape[0]  \n",
    "M_TEST = x_test_lstm.shape[0]             # number of test examples (2D),full=X_test.shape[0]\n",
    "N = x_train_lstm.shape[2]                 # number of features\n",
    "\n",
    "#BATCH = M_TRAIN                          # batch size\n",
    "DP = 1                            # dropout rate\n",
    "RDP = 1                          # recurrent dropout rate\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "# Build the Model\n",
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(LSTM(input_shape=(T, N), units=LAYERS[0],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "             #  dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_2.add(Dropout(dropout))\n",
    "model_2.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_2.add(LSTM(units=LAYERS[1],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "             #  dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_2.add(Dropout(dropout))\n",
    "model_2.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_2.add(LSTM(units=LAYERS[2],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              # dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=False, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_2.add(Dropout(dropout))\n",
    "model_2.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_2.add(Dense(units=LAYERS[3], activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model_2.compile(loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m], optimizer=Adam(lr=LR))\n",
    "\n",
    "print(model_2.summary())\n",
    "\n",
    "# Define a learning rate decay method:\n",
    "lr_decay = ReduceLROnPlateau(monitor='loss', \n",
    "                             patience=1, verbose=0, \n",
    "                             factor=0.5, min_lr=1e-8)\n",
    "\n",
    "# Define Early Stopping:\n",
    "early_stop = EarlyStopping(monitor='val_acc', min_delta=0, \n",
    "                           patience=30, verbose=1, mode='auto',\n",
    "                           baseline=0, restore_best_weights=True)\n",
    "# Train the model. \n",
    "# The dataset is small for NN - let's use test_data for validation\n",
    "start = time()\n",
    "\n",
    "##################################################\n",
    "\n",
    "History = model_2.fit(x_train_lstm_new, y_train_lstm_new,\n",
    "                    epochs=EPOCH,\n",
    "                    batch_size=BATCH,\n",
    "                    validation_split=0,\n",
    "                    validation_data=(x_valid_lstm_new[:M_VALIDATION], y_valid_lstm_new[:M_VALIDATION]),\n",
    "                    #validation_data=(x_test_lstm[:M_TEST], y_test[:M_TEST]),\n",
    "                    shuffle=True,\n",
    "                    verbose=2,\n",
    "                    callbacks=[lr_decay, early_stop])\n",
    "\n",
    "print('-'*65)\n",
    "print(f'Training was completed in {time() - start:.2f} secs')\n",
    "print('-'*65)\n",
    "# Evaluate the model:\n",
    "train_loss, train_acc, train_f1_score, train_precision = model_2.evaluate(x_train_lstm_new, y_train_lstm_new,\n",
    "                                       batch_size=BATCH, verbose=0)\n",
    "\n",
    "test_loss, test_acc, test_f1_score, test_precision = model_2.evaluate(x_test_lstm[:M_TEST], y_test[:M_TEST],\n",
    "                                     batch_size=BATCH, verbose=0)\n",
    "print('-'*65)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "print(f'test error = {round((1 - test_acc) * M_TEST)} out of {M_TEST} examples')\n",
    "\n",
    "y_pred2= model_2.predict(x_test_lstm)\n",
    "predict_train_lstm2=model_2.predict(x_train_lstm)\n",
    "\n",
    "test_acc_2=test_acc\n",
    "test_precision_2=test_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_2=[]\n",
    "for i in range(y_pred2.shape[0]): \n",
    "    if y_pred2[i]>0.5:\n",
    "        predict_test_2.append(1)\n",
    "    else:\n",
    "        predict_test_2.append(0)\n",
    "predict_test_2 = np.array(predict_test_2)\n",
    "print(predict_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1D=np.array(y_test).reshape(total_test)\n",
    "\n",
    "#predict_train_lstm = model.predict(x_train_lstm)\n",
    "#predict_train_lstm=np.array(predict_train_lstm).reshape(total_train.shape[0]) #37536\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test_2,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test_2)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "ppv =  cm1[1,1]/(cm1[0,1]+cm1[1,1])   \n",
    "npv =  cm1[0,0]/(cm1[0,0]+cm1[1,0])   \n",
    "\n",
    "print('specificity:',specificity)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('ppv:',ppv)\n",
    "print('npv:',npv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_event_0=x_train_lstm[:train_control]   #切出正常組 事件組   #loo3\n",
    "y_event_0=y_train[:train_control]\n",
    "\n",
    "x_event_1=x_train_lstm[train_control:]\n",
    "y_event_1=y_train[train_control:]\n",
    "\n",
    "part_0=int(int(x_event_0.shape[0])/5)\n",
    "part_1=int(int(x_event_1.shape[0])/5)\n",
    "\n",
    "x_train_lstm_new=np.concatenate((x_event_0[:2*part_0], x_event_1[:2*part_1],x_event_0[3*part_0:],x_event_1[3*part_1:]))\n",
    "x_valid_lstm_new=np.concatenate((x_event_0[2*part_0:3*part_0], x_event_1[2*part_1:3*part_1]))\n",
    "\n",
    "y_train_lstm_new=np.concatenate((y_event_0[:2*part_0], y_event_1[:2*part_1],y_event_0[3*part_0:], y_event_1[3*part_1:]))\n",
    "y_valid_lstm_new=np.concatenate((y_event_0[2*part_0:3*part_0], y_event_1[2*part_1:3*part_1]))\n",
    "\n",
    "print(x_train_lstm_new.shape)\n",
    "print(x_valid_lstm_new.shape)\n",
    "\n",
    "print(y_train_lstm_new.shape)\n",
    "print(y_valid_lstm_new.shape)\n",
    "\n",
    "LAYERS = [8,8,8,1]                # number of units in hidden and output layers\n",
    "M_TRAIN = x_train_lstm_new.shape[0]           # number of training examples (2D)\n",
    "M_VALIDATION =x_valid_lstm_new.shape[0]  \n",
    "M_TEST = x_test_lstm.shape[0]             # number of test examples (2D),full=X_test.shape[0]\n",
    "N = x_train_lstm.shape[2]                 # number of features\n",
    "\n",
    "#BATCH = M_TRAIN                          # batch size\n",
    "DP = 1                            # dropout rate\n",
    "RDP = 1                          # recurrent dropout rate\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "# Build the Model\n",
    "model_3 = Sequential()\n",
    "\n",
    "model_3.add(LSTM(input_shape=(T, N), units=LAYERS[0],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              # dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_3.add(Dropout(dropout))\n",
    "model_3.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_3.add(LSTM(units=LAYERS[1],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "             #  dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_3.add(Dropout(dropout))\n",
    "model_3.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_3.add(LSTM(units=LAYERS[2],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "             #  dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=False, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_3.add(Dropout(dropout))\n",
    "model_3.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_3.add(Dense(units=LAYERS[3], activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model_3.compile(loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m], optimizer=Adam(lr=LR))\n",
    "\n",
    "print(model_3.summary())\n",
    "\n",
    "# Define a learning rate decay method:\n",
    "lr_decay = ReduceLROnPlateau(monitor='loss', \n",
    "                             patience=1, verbose=0, \n",
    "                             factor=0.5, min_lr=1e-8)\n",
    "\n",
    "# Define Early Stopping:\n",
    "early_stop = EarlyStopping(monitor='val_acc', min_delta=0, \n",
    "                           patience=30, verbose=1, mode='auto',\n",
    "                           baseline=0, restore_best_weights=True)\n",
    "# Train the model. \n",
    "# The dataset is small for NN - let's use test_data for validation\n",
    "start = time()\n",
    "\n",
    "##################################################\n",
    "\n",
    "History = model_3.fit(x_train_lstm_new, y_train_lstm_new,\n",
    "                    epochs=EPOCH,\n",
    "                    batch_size=BATCH,\n",
    "                    validation_split=0,\n",
    "                    validation_data=(x_valid_lstm_new[:M_VALIDATION], y_valid_lstm_new[:M_VALIDATION]),\n",
    "                    #validation_data=(x_test_lstm[:M_TEST], y_test[:M_TEST]),\n",
    "                    shuffle=True,\n",
    "                    verbose=2,\n",
    "                    callbacks=[lr_decay, early_stop])\n",
    "\n",
    "print('-'*65)\n",
    "print(f'Training was completed in {time() - start:.2f} secs')\n",
    "print('-'*65)\n",
    "# Evaluate the model:\n",
    "train_loss, train_acc, train_f1_score, train_precision = model_3.evaluate(x_train_lstm_new, y_train_lstm_new,\n",
    "                                       batch_size=BATCH, verbose=0)\n",
    "\n",
    "test_loss, test_acc, test_f1_score, test_precision = model_3.evaluate(x_test_lstm[:M_TEST], y_test[:M_TEST],\n",
    "                                     batch_size=BATCH, verbose=0)\n",
    "print('-'*65)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "print(f'test error = {round((1 - test_acc) * M_TEST)} out of {M_TEST} examples')\n",
    "\n",
    "y_pred3= model_3.predict(x_test_lstm)\n",
    "predict_train_lstm3=model_3.predict(x_train_lstm)\n",
    "\n",
    "test_acc_3=test_acc\n",
    "test_precision_3=test_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_3=[]\n",
    "for i in range(y_pred3.shape[0]): \n",
    "    if y_pred3[i]>0.5:\n",
    "        predict_test_3.append(1)\n",
    "    else:\n",
    "        predict_test_3.append(0)\n",
    "predict_test_3 = np.array(predict_test_3)\n",
    "print(predict_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1D=np.array(y_test).reshape(total_test)\n",
    "\n",
    "#predict_train_lstm = model.predict(x_train_lstm)\n",
    "#predict_train_lstm=np.array(predict_train_lstm).reshape(total_train.shape[0]) #37536\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test_3,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test_3)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "ppv =  cm1[1,1]/(cm1[0,1]+cm1[1,1])   \n",
    "npv =  cm1[0,0]/(cm1[0,0]+cm1[1,0])  \n",
    "\n",
    "print('specificity:',specificity)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('ppv:',ppv)\n",
    "print('npv:',npv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_event_0=x_train_lstm[:train_control]   #切出正常組 事件組   #loo4\n",
    "y_event_0=y_train[:train_control]\n",
    "\n",
    "x_event_1=x_train_lstm[train_control:]\n",
    "y_event_1=y_train[train_control:]\n",
    "\n",
    "part_0=int(int(x_event_0.shape[0])/5)\n",
    "part_1=int(int(x_event_1.shape[0])/5)\n",
    "\n",
    "x_train_lstm_new=np.concatenate((x_event_0[:3*part_0], x_event_1[:3*part_1],x_event_0[4*part_0:],x_event_1[4*part_1:]))\n",
    "x_valid_lstm_new=np.concatenate((x_event_0[3*part_0:4*part_0], x_event_1[3*part_1:4*part_1]))\n",
    "\n",
    "y_train_lstm_new=np.concatenate((y_event_0[:3*part_0], y_event_1[:3*part_1],y_event_0[4*part_0:], y_event_1[4*part_1:]))\n",
    "y_valid_lstm_new=np.concatenate((y_event_0[3*part_0:4*part_0], y_event_1[3*part_1:4*part_1]))\n",
    "\n",
    "print(x_train_lstm_new.shape)\n",
    "print(x_valid_lstm_new.shape)\n",
    "\n",
    "print(y_train_lstm_new.shape)\n",
    "print(y_valid_lstm_new.shape)\n",
    "\n",
    "\n",
    "LAYERS = [8,8,8,1]                # number of units in hidden and output layers\n",
    "M_TRAIN = x_train_lstm_new.shape[0]           # number of training examples (2D)\n",
    "M_VALIDATION =x_valid_lstm_new.shape[0]  \n",
    "M_TEST = x_test_lstm.shape[0]             # number of test examples (2D),full=X_test.shape[0]\n",
    "N = x_train_lstm.shape[2]                 # number of features\n",
    "\n",
    "#BATCH = M_TRAIN                          # batch size\n",
    "DP = 1                            # dropout rate\n",
    "RDP = 1                          # recurrent dropout rate\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "# Build the Model\n",
    "model_4 = Sequential()\n",
    "\n",
    "model_4.add(LSTM(input_shape=(T, N), units=LAYERS[0],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              # dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_4.add(Dropout(dropout))\n",
    "model_4.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_4.add(LSTM(units=LAYERS[1],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "            #   dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_4.add(Dropout(dropout))\n",
    "model_4.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_4.add(LSTM(units=LAYERS[2],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "           #    dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=False, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_4.add(Dropout(dropout))\n",
    "model_4.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_4.add(Dense(units=LAYERS[3], activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model_4.compile(loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m], optimizer=Adam(lr=LR))\n",
    "\n",
    "print(model_4.summary())\n",
    "\n",
    "# Define a learning rate decay method:\n",
    "lr_decay = ReduceLROnPlateau(monitor='loss', \n",
    "                             patience=1, verbose=0, \n",
    "                             factor=0.5, min_lr=1e-8)\n",
    "\n",
    "# Define Early Stopping:\n",
    "early_stop = EarlyStopping(monitor='val_acc', min_delta=0, \n",
    "                           patience=30, verbose=1, mode='auto',\n",
    "                           baseline=0, restore_best_weights=True)\n",
    "# Train the model. \n",
    "# The dataset is small for NN - let's use test_data for validation\n",
    "start = time()\n",
    "\n",
    "##################################################\n",
    "\n",
    "History = model_4.fit(x_train_lstm_new, y_train_lstm_new,\n",
    "                    epochs=EPOCH,\n",
    "                    batch_size=BATCH,\n",
    "                    validation_split=0,\n",
    "                    validation_data=(x_valid_lstm_new[:M_VALIDATION], y_valid_lstm_new[:M_VALIDATION]),\n",
    "                    #validation_data=(x_test_lstm[:M_TEST], y_test[:M_TEST]),\n",
    "                    shuffle=True,\n",
    "                    verbose=2,\n",
    "                    callbacks=[lr_decay, early_stop])\n",
    "\n",
    "print('-'*65)\n",
    "print(f'Training was completed in {time() - start:.2f} secs')\n",
    "print('-'*65)\n",
    "# Evaluate the model:\n",
    "train_loss, train_acc, train_f1_score, train_precision = model_4.evaluate(x_train_lstm_new, y_train_lstm_new,\n",
    "                                       batch_size=BATCH, verbose=0)\n",
    "\n",
    "test_loss, test_acc, test_f1_score, test_precision = model_4.evaluate(x_test_lstm[:M_TEST], y_test[:M_TEST],\n",
    "                                     batch_size=BATCH, verbose=0)\n",
    "print('-'*65)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "print(f'test error = {round((1 - test_acc) * M_TEST)} out of {M_TEST} examples')\n",
    "\n",
    "y_pred4= model_4.predict(x_test_lstm)\n",
    "predict_train_lstm4=model_4.predict(x_train_lstm)\n",
    "\n",
    "test_acc_4=test_acc\n",
    "test_precision_4=test_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_4=[]\n",
    "for i in range(y_pred4.shape[0]): \n",
    "    if y_pred4[i]>0.5:\n",
    "        predict_test_4.append(1)\n",
    "    else:\n",
    "        predict_test_4.append(0)\n",
    "predict_test_4 = np.array(predict_test_4)\n",
    "print(predict_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1D=np.array(y_test).reshape(total_test)\n",
    "\n",
    "#predict_train_lstm = model.predict(x_train_lstm)\n",
    "#predict_train_lstm=np.array(predict_train_lstm).reshape(total_train.shape[0]) #37536\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test_4,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test_4)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "ppv =  cm1[1,1]/(cm1[0,1]+cm1[1,1])   \n",
    "npv =  cm1[0,0]/(cm1[0,0]+cm1[1,0])   \n",
    "\n",
    "print('specificity:',specificity)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('ppv:',ppv)\n",
    "print('npv:',npv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_event_0=x_train_lstm[:train_control]   #切出正常組 事件組   #loo5\n",
    "y_event_0=y_train[:train_control]\n",
    "\n",
    "x_event_1=x_train_lstm[train_control:]\n",
    "y_event_1=y_train[train_control:]\n",
    "\n",
    "part_0=int(int(x_event_0.shape[0])/5)\n",
    "part_1=int(int(x_event_1.shape[0])/5)\n",
    "\n",
    "x_train_lstm_new=np.concatenate((x_event_0[:4*part_0], x_event_1[:4*part_1]))\n",
    "x_valid_lstm_new=np.concatenate((x_event_0[4*part_0:], x_event_1[4*part_1:]))\n",
    "\n",
    "y_train_lstm_new=np.concatenate((y_event_0[:4*part_0], y_event_1[:4*part_1]))\n",
    "y_valid_lstm_new=np.concatenate((y_event_0[4*part_0:], y_event_1[4*part_1:]))\n",
    "\n",
    "print(x_train_lstm_new.shape)\n",
    "print(x_valid_lstm_new.shape)\n",
    "\n",
    "print(y_train_lstm_new.shape)\n",
    "print(y_valid_lstm_new.shape)\n",
    "\n",
    "LAYERS = [8,8,8,1]                # number of units in hidden and output layers\n",
    "M_TRAIN = x_train_lstm_new.shape[0]           # number of training examples (2D)\n",
    "M_VALIDATION =x_valid_lstm_new.shape[0]  \n",
    "M_TEST = x_test_lstm.shape[0]             # number of test examples (2D),full=X_test.shape[0]\n",
    "N = x_train_lstm.shape[2]                 # number of features\n",
    "\n",
    "#BATCH = M_TRAIN                          # batch size\n",
    "DP = 1                            # dropout rate\n",
    "RDP = 1                          # recurrent dropout rate\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "# Build the Model\n",
    "model_5 = Sequential()\n",
    "\n",
    "model_5.add(LSTM(input_shape=(T, N), units=LAYERS[0],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "            #   dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_5.add(Dropout(dropout))\n",
    "model_5.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_5.add(LSTM(units=LAYERS[1],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "            #   dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_5.add(Dropout(dropout))\n",
    "model_5.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_5.add(LSTM(units=LAYERS[2],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "           #    dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=False, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model_5.add(Dropout(dropout))\n",
    "model_5.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_5.add(Dense(units=LAYERS[3], activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model_5.compile(loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m], optimizer=Adam(lr=LR))\n",
    "\n",
    "print(model_5.summary())\n",
    "\n",
    "# Define a learning rate decay method:\n",
    "lr_decay = ReduceLROnPlateau(monitor='loss', \n",
    "                             patience=1, verbose=0, \n",
    "                             factor=0.5, min_lr=1e-8)\n",
    "\n",
    "# Define Early Stopping:\n",
    "early_stop = EarlyStopping(monitor='val_acc', min_delta=0, \n",
    "                           patience=30, verbose=1, mode='auto',\n",
    "                           baseline=0, restore_best_weights=True)\n",
    "# Train the model. \n",
    "# The dataset is small for NN - let's use test_data for validation\n",
    "start = time()\n",
    "\n",
    "##################################################\n",
    "\n",
    "History = model_5.fit(x_train_lstm_new, y_train_lstm_new,\n",
    "                    epochs=EPOCH,\n",
    "                    batch_size=BATCH,\n",
    "                    validation_split=0,\n",
    "                    validation_data=(x_valid_lstm_new[:M_VALIDATION], y_valid_lstm_new[:M_VALIDATION]),\n",
    "                    #validation_data=(x_test_lstm[:M_TEST], y_test[:M_TEST]),\n",
    "                    shuffle=True,\n",
    "                    verbose=2,\n",
    "                    callbacks=[lr_decay, early_stop])\n",
    "\n",
    "print('-'*65)\n",
    "print(f'Training was completed in {time() - start:.2f} secs')\n",
    "print('-'*65)\n",
    "# Evaluate the model:\n",
    "train_loss, train_acc, train_f1_score, train_precision = model_5.evaluate(x_train_lstm_new, y_train_lstm_new,\n",
    "                                       batch_size=BATCH, verbose=0)\n",
    "\n",
    "test_loss, test_acc, test_f1_score, test_precision = model_5.evaluate(x_test_lstm[:M_TEST], y_test[:M_TEST],\n",
    "                                     batch_size=BATCH, verbose=0)\n",
    "print('-'*65)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "print(f'test error = {round((1 - test_acc) * M_TEST)} out of {M_TEST} examples')\n",
    "\n",
    "y_pred5= model_5.predict(x_test_lstm)\n",
    "predict_train_lstm5=model_5.predict(x_train_lstm)\n",
    "\n",
    "test_acc_5=test_acc\n",
    "test_precision_5=test_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_5=[]\n",
    "for i in range(y_pred5.shape[0]): \n",
    "    if y_pred5[i]>0.5:\n",
    "        predict_test_5.append(1)\n",
    "    else:\n",
    "        predict_test_5.append(0)\n",
    "predict_test_5 = np.array(predict_test_5)\n",
    "print(predict_test_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1D=np.array(y_test).reshape(total_test)\n",
    "\n",
    "#predict_train_lstm = model.predict(x_train_lstm)\n",
    "#predict_train_lstm=np.array(predict_train_lstm).reshape(total_train.shape[0]) #37536\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test_5,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test_5)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "ppv =  cm1[1,1]/(cm1[0,1]+cm1[1,1])   \n",
    "npv =  cm1[0,0]/(cm1[0,0]+cm1[1,0])  \n",
    "\n",
    "print('specificity:',specificity)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('ppv:',ppv)\n",
    "print('npv:',npv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_temp=np.append(y_pred1,y_pred2)\n",
    "y_pred_temp=np.append(y_pred_temp,y_pred3)\n",
    "y_pred_temp=np.append(y_pred_temp,y_pred4)\n",
    "y_pred_temp=np.append(y_pred_temp,y_pred5)\n",
    "\n",
    "predict_train_temp=np.append(predict_train_lstm1,predict_train_lstm2)\n",
    "predict_train_temp=np.append(predict_train_temp,predict_train_lstm3)\n",
    "predict_train_temp=np.append(predict_train_temp,predict_train_lstm4)\n",
    "predict_train_temp=np.append(predict_train_temp,predict_train_lstm5)\n",
    "\n",
    "y_pred=np.array(y_pred_temp).reshape(x_test_lstm.shape[0],5, order='F') #轉維\n",
    "predict_train_lstm=np.array(predict_train_temp).reshape(x_train_lstm.shape[0],5, order='F') #轉維\n",
    "\n",
    "y_pred= np.mean(y_pred, axis=1)\n",
    "predict_train_lstm= np.mean(predict_train_lstm, axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test=[]\n",
    "for i in range(y_pred.shape[0]): \n",
    "    if y_pred[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "print(predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1D=np.array(y_test).reshape(total_test)\n",
    "\n",
    "#predict_train_lstm = model.predict(x_train_lstm)\n",
    "#predict_train_lstm=np.array(predict_train_lstm).reshape(total_train.shape[0]) #37536\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "\n",
    "accuracy_5_fold=(cm1[0,0]+cm1[1,1])/(cm1[1,1]+cm1[0,0]+cm1[0,1]+cm1[1,0])\n",
    "\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "ppv =  cm1[1,1]/(cm1[0,1]+cm1[1,1])   \n",
    "npv =  cm1[0,0]/(cm1[0,0]+cm1[0,1])  \n",
    "\n",
    "print('specificity:',specificity)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('ppv:',ppv)\n",
    "print('npv:',npv)\n",
    "\n",
    "\n",
    "y_pred=np.array(y_pred).reshape(total_test)\n",
    "\n",
    "flag=0\n",
    "total_predict=0\n",
    "for i in range(y_pred.shape[0]): \n",
    "      if y_pred[i]>0.5:\n",
    "            total_predict=total_predict+y_pred[i]\n",
    "            flag=flag+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr,tpr,threshold = roc_curve(y_test, y_pred) ###計算真正率和假正率\n",
    "roc_auc = auc(fpr,tpr) ###計算auc的值\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "lw=lw, label='LSTM ROC curve (area = %0.2f)' % roc_auc) ###假正率為橫座標，真正率為縱座標做曲線\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('5 fold LSTM')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy : %0.2f' %accuracy_5_fold)  #accuracy\n",
    "print('AUC : %0.2f' % roc_auc)  #AUC\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "#print('f1_score :%0.2f' %test_f1_score)  #f1_score\n",
    "print(total_predict/flag*100)  #score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics,ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import model_selection\n",
    "\n",
    "forest = ensemble.RandomForestClassifier()\n",
    "\n",
    "rf_params = {\n",
    "'n_estimators': [15,20,25],\n",
    "'max_depth': [4,5,6,7]\n",
    " }\n",
    "\n",
    "forest = model_selection.GridSearchCV(forest, rf_params, cv=5)\n",
    "forest = forest.fit(x_train_base, y_train)\n",
    "\n",
    "prob_predict_y_validation1 = forest.predict_proba(x_train_base)#给出带有概率值的结果，每个点所有label的概率和为1\n",
    "prob_predict_y_validation = forest.predict_proba(x_test_base)#给出带有概率值的结果，每个点所有label的概率和为1\n",
    "\n",
    "\n",
    "y_score = prob_predict_y_validation[:, 1]\n",
    "# 預測\n",
    "predict_train_rf = prob_predict_y_validation1[:, 1]\n",
    "\n",
    "test_y_predicted = forest.predict(x_test_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc  ###計算roc和auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "def roc_curve_and_score(y_test, pred_proba):\n",
    "    fpr, tpr, _ = roc_curve(y_test.ravel(), pred_proba.ravel())\n",
    "    roc_auc = roc_auc_score(y_test.ravel(), pred_proba.ravel())\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "#plt.grid()\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, y_score)\n",
    "plt.plot(fpr, tpr, color='#00db00', lw=2,\n",
    "         label='Cardiac AUC={0:.2f}'.format(roc_auc))\n",
    "\n",
    "plt.title('Random Forest')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test=[]\n",
    "for i in range(y_score.shape[0]): \n",
    "    if y_score[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "y_test_1D=np.array(y_test).reshape(total_test)\n",
    "\n",
    "pd.crosstab(y_test_1D,predict_test,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict_test)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "\n",
    "print('AUC : %0.2f' % roc_auc)  #AUC\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train_base, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train_logistic_result = logreg.predict_proba(x_train_base)\n",
    "\n",
    "predict_train_logistic = predict_train_logistic_result[:, 1]\n",
    "\n",
    "logreg_test_y_predicted = logreg.predict_proba(x_test_base)\n",
    "\n",
    "log_y_score = logreg_test_y_predicted[:, 1]\n",
    "\n",
    "predict_test=[]\n",
    "for i in range(log_y_score.shape[0]): \n",
    "    if log_y_score[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "#print(predict_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score,confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test,predict_test)\n",
    "\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "\n",
    "#print('Precision:',precision_score(y_test_1, predict_test))\n",
    "#print('Recall:', recall_score(y_test_1, predict_test))\n",
    "#print('f1-score: %f' % f1_score(y_test_1, predict_test))\n",
    "print('Accuracy: %f' % accuracy_score(y_test, predict_test))\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "pd.crosstab(y_test_log,predict_test,rownames=['label'],colnames=['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc  ###計算roc和auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "def roc_curve_and_score(y_test, pred_proba):\n",
    "    fpr, tpr, _ = roc_curve(y_test.ravel(), pred_proba.ravel())\n",
    "    roc_auc = roc_auc_score(y_test.ravel(), pred_proba.ravel())\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "#plt.grid()\n",
    "fpr, tpr, roc_auc = roc_curve_and_score(y_test, log_y_score)\n",
    "plt.plot(fpr, tpr, color='#00db00', lw=2,\n",
    "         label='Cardiac AUC={0:.2f}'.format(roc_auc))\n",
    "\n",
    "plt.title('logistic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_train_logistic)\n",
    "print(predict_train_lstm)\n",
    "\n",
    "stacking=np.append(predict_train_logistic, predict_train_lstm)\n",
    "x_train_stacking=np.array(stacking).reshape(x_train_lstm.shape[0],2, order='F') #轉維\n",
    "\n",
    "from sklearn import  svm, preprocessing, metrics \n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "svm = svm.SVC(kernel='linear',probability=True)\n",
    "svm.fit(x_train_stacking,y_train)\n",
    "\n",
    "print(x_train_stacking.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(log_y_score.shape)#logistic test 機率\n",
    "print(y_pred.shape)#lstm test 機率 \n",
    "print(y_score.shape)#Rf test 機率\n",
    "\n",
    "stacking_test=np.append(y_pred, log_y_score)\n",
    "x_test_stacking=np.array(stacking_test).reshape(total_test,2, order='F') #轉維\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=svm.predict(x_test_stacking)\n",
    "predict_pro=svm.predict_proba(x_test_stacking)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, predict)\n",
    "precision  = metrics.precision_score(y_test, predict)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "\n",
    "pd.crosstab(y_test_1D,predict,rownames=['label'],colnames=['predict'])\n",
    "predict_pro=predict_pro[:,1:2]\n",
    "\n",
    "#################92個test ca 輸出#####################\n",
    "#test=pd.DataFrame(predict[4689:])\n",
    "#test.to_csv('24hour_ca.csv', index=False)\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test_1D,predict_test,rownames=['label'],colnames=['predict'])  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_1D,predict)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "#####from confusion matrix calculate \n",
    "\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "ppv =  cm1[1,1]/(cm1[0,1]+cm1[1,1])   \n",
    "npv =  cm1[0,0]/(cm1[0,0]+cm1[0,1])  \n",
    "\n",
    "print('specificity:',specificity)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('ppv:',ppv)\n",
    "print('npv:',npv)\n",
    "flag=0\n",
    "total_predict=0\n",
    "for i in range(y_pred.shape[0]): \n",
    "      if predict_pro[i]>0.5:\n",
    "            total_predict=total_predict+predict_pro[i]\n",
    "            flag=flag+1\n",
    "#print(flag)  #score\n",
    "\n",
    "print(y_test_1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr,tpr,threshold = roc_curve(y_test, predict_pro) ###計算真正率和假正率\n",
    "roc_auc = auc(fpr,tpr) ###計算auc的值\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) ###假正率為橫座標，真正率為縱座標做曲線\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('stacking LSTM')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy : %0.2f' %accuracy)  #Accuracy\n",
    "print('precision : %0.2f' %precision)  #precision\n",
    "print('AUC : %0.2f' % roc_auc)  #AUC\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "#print('f1_score :%0.2f' %test_f1_score)  #f1_score\n",
    "print(total_predict/flag*100)  #score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
