{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7305, 256, 256, 3)\n",
      "(1502, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential, load_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "cxr=pd.read_csv(\"cxr_combine_mutli_ca.csv\")\n",
    "count=0\n",
    "train_x = [] \n",
    "test_x = [] \n",
    "train_y = [] \n",
    "test_y = [] \n",
    "\n",
    "for index, row in cxr.iterrows():\n",
    "     if count<7305:\n",
    "            img = cv2.imread(\"my_single_mutli_ca\" + \"/\" + row[\"dicom_id\"]+\".jpg\")\n",
    "           # img = cv2.resize(img, (64, 64)) \n",
    "            train_x.append(img)\n",
    "            train_y.append(row[\"eventV3\"])\n",
    "            count=count+1\n",
    "     else :\n",
    "         img = cv2.imread(\"my_single_mutli_ca\" + \"/\" + row[\"dicom_id\"]+\".jpg\")\n",
    "         #img = cv2.resize(img, (64, 64)) \n",
    "         test_x.append(img)\n",
    "         test_y.append(row[\"eventV3\"])\n",
    "         count=count+1\n",
    "\n",
    "train_x=np.array(train_x)\n",
    "test_x=np.array(test_x)\n",
    "\n",
    "#train_x = train_x.reshape(5887, 256*256*3).astype('float32')\n",
    "#test_x = test_x.reshape(1502, 256*256*3).astype('float32')\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "#print(train_y)\n",
    "#print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3100, 256, 256, 3)\n",
      "(3100,)\n"
     ]
    }
   ],
   "source": [
    "train_x_temp=np.concatenate((train_x[:1550], train_x[-1550:]))\n",
    "train_y_temp=np.concatenate((train_y[:1550], train_y[-1550:]))\n",
    "\n",
    "print(train_x_temp.shape)\n",
    "print(train_y_temp.shape)\n",
    "\n",
    "train_x_temp=train_x_temp/256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.astype('float32')/255\n",
    "test_x = test_x.astype('float32')/255\n",
    "\n",
    "#print(train_y)\n",
    "#print(test_y)\n",
    "\n",
    "#train_y = np_utils.to_categorical(train_y)\n",
    "#test_y = np_utils.to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7305,)\n",
      "(1502,)\n"
     ]
    }
   ],
   "source": [
    "train_y=np.array(train_y)\n",
    "test_y=np.array(test_y)\n",
    "\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=Sequential()\n",
    "\n",
    "#model.add(Dense(units=50,input_dim=196608,kernel_initializer='uniform',activation='relu')) \n",
    "#model.add(Dense(units=50,input_shape=(512, 512, 3),kernel_initializer='uniform',activation='relu')) \n",
    "#model.add(Dense(units=10,kernel_initializer='uniform',activation='relu'))\n",
    "#model.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n",
    "\n",
    "#model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_history = model.fit(x=train_x, y=train_y, validation_split=0.2, epochs=5, batch_size=8, verbose=2)\n",
    "\n",
    "# 顯示訓練成果(分數)\n",
    "#scores = model.evaluate(test_x, test_y)\n",
    "#print()\n",
    "#print(\"\\t[Info] Accuracy of testing data = {:2.1f}%\".format(scores[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neur_test_y_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neur_test_y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2000)              4098000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2000)              8000      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2001      \n",
      "=================================================================\n",
      "Total params: 4,396,801\n",
      "Trainable params: 4,391,905\n",
      "Non-trainable params: 4,896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "####################建立model#########################\n",
    "from keras.layers import Dense, LSTM, BatchNormalization\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=3, input_shape=(256, 256, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=32, kernel_size=3, input_shape=(256, 256, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(8,8)))\n",
    "          \n",
    "model.add(Conv2D(filters=64, kernel_size=3, input_shape=(256, 256, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=3, input_shape=(256, 256, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(4,4)))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=3, input_shape=(256, 256, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=128, kernel_size=3, input_shape=(256, 256, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "####################建立model#########################\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) #binary_crossentropy categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "310/310 - 29s - loss: 0.9557 - accuracy: 0.3750 - val_loss: 1.8345 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "310/310 - 6s - loss: 0.6698 - accuracy: 0.3750 - val_loss: 2.4191 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "310/310 - 6s - loss: 0.6635 - accuracy: 0.3750 - val_loss: 2.0974 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "310/310 - 6s - loss: 0.6058 - accuracy: 0.3750 - val_loss: 0.5135 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "310/310 - 6s - loss: 0.6027 - accuracy: 0.3750 - val_loss: 2.4231 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(x=train_x_temp, y=train_y_temp, validation_split=0.2, epochs=5, batch_size=8, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neur_train_y_predicted = model.predict(train_x_temp)\n",
    "predict_test=[]\n",
    "for i in range(neur_train_y_predicted.shape[0]): \n",
    "    if neur_train_y_predicted[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "#print(predict_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score,confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(train_y_temp,predict_test)\n",
    "\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "\n",
    "#print('Precision:',precision_score(y_test_1, predict_test))\n",
    "#print('Recall:', recall_score(y_test_1, predict_test))\n",
    "#print('f1-score: %f' % f1_score(y_test_1, predict_test))\n",
    "print('Accuracy: %f' % accuracy_score(train_y_temp, predict_test))\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "pd.crosstab(train_y_temp,predict_test,rownames=['label'],colnames=['predict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.021305\n",
      "Sensitivity : 1.00\n",
      "Specificity :0.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predict</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predict     1\n",
       "label        \n",
       "0        1470\n",
       "1          32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neur_test_y_predicted = model.predict(test_x)\n",
    "predict_test=[]\n",
    "for i in range(neur_test_y_predicted.shape[0]): \n",
    "    if neur_test_y_predicted[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "#print(predict_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score,confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(test_y,predict_test)\n",
    "\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "\n",
    "#print('Precision:',precision_score(y_test_1, predict_test))\n",
    "#print('Recall:', recall_score(y_test_1, predict_test))\n",
    "#print('f1-score: %f' % f1_score(y_test_1, predict_test))\n",
    "print('Accuracy: %f' % accuracy_score(test_y, predict_test))\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "pd.crosstab(test_y,predict_test,rownames=['label'],colnames=['predict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.212183\n",
      "Sensitivity : 1.00\n",
      "Specificity :0.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predict</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predict     1\n",
       "label        \n",
       "0        5755\n",
       "1        1550"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neur_train_y_predicted = model.predict(train_x)\n",
    "predict_test=[]\n",
    "for i in range(neur_train_y_predicted.shape[0]): \n",
    "    if neur_train_y_predicted[i]>0.5:\n",
    "        predict_test.append(1)\n",
    "    else:\n",
    "        predict_test.append(0)\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "#print(predict_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score,confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(train_y,predict_test)\n",
    "\n",
    "sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])   #TPR\n",
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])   #FPR\n",
    "\n",
    "#print('Precision:',precision_score(y_test_1, predict_test))\n",
    "#print('Recall:', recall_score(y_test_1, predict_test))\n",
    "#print('f1-score: %f' % f1_score(y_test_1, predict_test))\n",
    "print('Accuracy: %f' % accuracy_score(train_y, predict_test))\n",
    "print('Sensitivity : %0.2f' % sensitivity )\n",
    "print('Specificity :%0.2f' % specificity)\n",
    "\n",
    "pd.crosstab(train_y,predict_test,rownames=['label'],colnames=['predict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output= pd.DataFrame(neur_test_y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
